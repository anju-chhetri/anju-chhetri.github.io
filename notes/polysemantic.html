<!DOCTYPE html>
<html>
  
<body>
        
<h2>Polysemantic Neurons</h2>
<i>While conducting a literature review, I came across a word that caught my attention and seemed quite interesting. 
    I thought it would be worthwhile to introduce some relevant terminologies here.</i>

<br>
<br>

When neurons in a neural network correspond to more than one feature, they are called <b>polysemantic neurons</b>. Conversely, if they correspond to only a single feature, they are referred to as <b>monosemantic neurons</b>. <br>
 <br>
 The word 'feature' wasn't easy for me to decode at first. While features can be described as properties interpretable by humans, it cannot be guaranteed that all features are understandable. What may appear as a random signal to a human could represent an important piece of information that human knowledge has failed to decode. 
 One definition of a feature in the context of neural networks that I find fulfilling is:
<br>
<br>
<center>
    Properties of the input which a sufficiently large neural network will reliably dedicate a neuron to representing. For example, curve detectors appear to reliably occur across sufficiently sophisticated vision models, and so are a feature. [1]
 </center>
<br>
One interesting finding was that polysemantic neurons tend to occur when the input data exhibits high kurtosis or sparsity. Additionally, their prevalence varies according to the type of architecture used [2]. Features with similar importance levels are often in superposition, whereas features with high importance are more commonly assigned to a dedicated neuron.

<h3>Polysemanticity and Superposition</h3>
<br>

<figure >
    <img class="notes-jpeg-name" id="me" src="/notes/images/polysemanticity/polyandsuper.png" style="width:500px;height:auto;">
    <figcaption>Fig 1: Visualization of polysemanticity and superposition state in neural networks</figcaption>
  </figure> 

<br>
While going through the literature, I found that these two terms may sound similar, but there is a sharp distinction between them. In polysemanticity, the learned features are not basis-aligned, even if there is an incentive to be so. 
In contrast, in the superposition state, the number of features encoded by the layer exceeds the number of dimensions it has. Therefore, I like to view superposition as a special case of polysemanticity.

<br>
<h3>Circuits</h3>
Circuits in neural networks are graphs responsible for performing a single task, composed of interconnected neurons that process input signals 
to produce output. To obtain these circuits, we can use techniques like Layer-wise Relevance Propagation (LRP), which operates by propagating predictions backward through the network. This method helps identify which input features are most relevant to the model's predictions.

<figure >
    <img class="notes-jpeg-name" id="me" src="/notes/images/polysemanticity/LRP.png" style="width:500px;height:auto;">
    <figcaption>Fig 2: Layer-wise Relevance Propagation (LRP) in action. (I forgot the website name but will try to remember and cite it.)</figcaption>
  </figure> 


<br>
Explainable AI demos: <a href="https://lrpserver.hhi.fraunhofer.de/" target="_blank">Here</a>
<br>

<h2>References</h2>
<ul>
    <li> Toy Models of Superposition | <a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank">Here</a></li>
    <li> Polysemanticity and Capacity in Neural Networks | <a href="https://arxiv.org/abs/2210.01892" target="_blank">Here</a></li>
  </ul>
<br>
<p class="notes-date">
<i>6th October, 2024</i>
</p>

   </body>

</html>
