<!DOCTYPE html>
<html>
  
<body>
    <i>
        This is my summary of the report on <a href="https://arxiv.org/abs/2501.16496" target="_blank">Open Problems in Mechanistic Interpretability</a>
    </i>
<h2>Open Problems in Mechanistic Interpretability</h2>

<b>Difference Between Interpretability and Mechanistic Interpretability</b>
    <ul>
    <li>Interpretability refers to methods for understanding neural networks from the inside out.</li>
    
    <li>Mechanistic Interpretability is a specific approach to interpretability that focuses on understanding the internal mechanisms of a neural network</li>
    </ul>

<b> Three Broad Approaches to Interpretability in AI: </b>
    <ul>
        <li><i>Interpretability by Design</i> – Creating models that are inherently more interpretable (e.g., decision trees, rule-based systems).</li>
        <li><i>Why Did the Model Make This Decision?</i></li>
        <li><i>How Does the Model Solve a General Class of Problems?</i> (Mechanistic Interpretability)</li>
    </ul>

<b>Mechanistic Interpretability – Core Goal </b>
<p>
The primary aim is to decompose a neural network and study its components in isolation.
This helps us explain how neural networks generalize and make decisions at a fundamental level.
By understanding the internal computations of neural networks we can safely deploy them in safety-critical and ethically-sensitive domains.
Additionally it also allows us to create and study aritifical minds with a level of access and control that is not possible with human minds.
</p>

<b>Two widely used method of performing mechanistic interpretability are: </b>
<ul>
    <li><b style="color: #1A5276;">Reverse Engineering: Decompose a network into components and then identify the role of each component.</b>
        <ul>
            <li><i>Why reverse engineer?</i> Humans and neural networks often use different representations. For example, while humans solve modular addition using simple carries, a small transformer model learned a Fourier transform strategy instead [1].</li>
            <li>Steps of Performing Reverse Engineering:
                <ul>
                    <li ><b style="color: #117A65;">Decomposition of Network into Smaller Components</b>
                        <ul style="list-style-type: disc;">
                            <li>
                                Individual neurons and attention heads often exhibit polysemanticity, and some research suggests that representations in language models can span across multiple layers. 
                                Therefore, decomposing neural network representations solely based on neurons, attention heads, or layers may not provide a natural or effective approach.
                                A common strategy is to provide the model with a range of unlabeled inputs, collect the resulting hidden activations, and then apply unsupervised dimensionality reduction techniques to those activations.
                                However, if neurons are in superposition (where more features are encoded than there are neurons), traditional dimensionality reduction techniques, such as Principal Component Analysis (PCA), may fail.
                            </li>
                            <li>
                                To address the challenge of superposition, methods like Sparse Dictionary Learning (SDL) are used.
                                SDL can represent more features than there are dimensions, as long as each feature activates sparsely.
                                Various methods fall under this umbrella, such as Sparse Autoencoders (SAE), Transcoders, and Crosscoders.
                                The primary goal in SDL is to train the dictionary elements to align with the ‘feature directions’ in the model’s activations.
                                The objective is typically to reconstruct input, the next layer's activations, or the activations across multiple layers simultaneously.
                            </li>
                            <li>
                                Limitations with Sparse Dictionary Learning (SDL):
                                <ul><i>
                                    <li>High reconstruction error.</li>
                                    <li>Does not provide clear information about the geometry of features.</li>
                                    <li>Primarily works with activations rather than providing information on the actual mechanisms behind the neural network's decision-making process.</li>
                                    <li>Scaling SDL methods to large models can be computationally expensive.
                                        The exact scaling behavior (whether it’s sub- or supra-linear) remains unclear.</li>
                                    <li>Sparsity in the model may not be a good proxy for interpretability.</li>
                                    <li>Assumes that features are represented in a linear way, which may not align with the non-linear nature of most neural network models.</li>
                                    <li>Might not capture the exact features we (humans) want to interpret, but this could reflect a discrepancy between our understanding of concepts and how the model processes them. In this case, the SDLs may still be functioning as intended.</li>
                                    <li>Cannot be straightforwardly applied to all types of architectures, limiting its universality.</li>
                                </i></ul>
                            </li>
                            <li>Given the practical and conceptual challenges with SDL, an important question arises: <b>How can we decompose a network into atomic units?</b> After investing considerable effort into SDL approaches, it’s clear that improving conceptual clarity beyond the concept of superposition is crucial to advancing neural network decomposition.</li>
                        </ul>
                    </li>
                    <li><b style="color: #117A65;">Interpretation: Formulating a Hypothesis about the Function of Each Component</b>
                    <ul style="list-style-type: disc;">
                        <li>After decomposing a network into components, the next step is to hypothesize the functional role of each component.</li>
                        <li>Two Approaches for Formulating Hypotheses:
                            <ul>
                                <li>
                                    <b style="color: #2874A6;">What Causes Their Activation?</b>
                                    <ul style="list-style-type: circle;">
                                        <li>Highly activating dataset examples</li>
                                        <li>Potential issues: Many of the issues with highly activating data set examples stem from the fact that they merely provide correlational explanations for the activation of a network component, rather than causal explanations.
                                            <ul>
                                                <li><b>Human Bias</b>: This method relies on human prior beliefs, which may lead interpreters to project human understanding onto models that could be using completely unfamiliar concepts.</li>
                                                <li><b>Interpretability Illusions</b>: There is a risk of constructing interpretability illusions, where plausible explanations based on dataset examples are mistaken for fundamental truths.</li>
                                                <li>Highly activating dataset examples cannot be solely relied upon to identify the <b>basic units of computation</b> in neural networks.</li>
                                            </ul>
                                        </li>
                                        <li>Attrbution Methods
                                            Many gradient-based methods identify only a <b>first-order approximation</b> of the ideal attribution, which is sometimes inaccurate. <i>Developing efficient and accurate attribution methods is an open problem in mechanistic interpretability.</i>
                                        </li>
                                        <li>Feature Synthesis
                                            Feature synthesis is a strategy that integrates highly activating dataset examples and gradient-based attribution methods to form more comprehensive hypotheses.
                                        </li>
                                    </ul>

                                </li>
                                <li> <b style="color: #2874A6;">What happens after that component has been activated?: <i>to be written...</i></b></li>
                            </ul>
                        </li>

                    </ul>
                    </li>
                <li> <b style="color: #117A65;">Validation of description: Test the hypothesis</b></li>
                </ul>
            </li>
        </ul>
    </li>
    <li><b style="color: #1A5276;">Concept-based interpretability: Find a set of concepts and then see which components contribute the activation of such concepts. Concepts can thought of as a feature of the data.</b></li>
</ul>

<h3>Open Questions:</h3>
<ul>
    <li>How valid is the superposition hypothesis? Is it fundamentally valid, or merely pragmatically useful?</li>

    <li>Theories of Generalization: Given the field’s objective to understand the learned structures behind neural networks' generalization behaviors, exploring theories about why networks generalize in the way they do seems promising. Thus, stronger theoretical foundations may also be essential for developing models that are intrinsically decomposable by design.</li>

</ul>
<h2>References</h2>
<ul>
    <li> Progress measures for grokking via mechanistic interpretability| <a href="https://arxiv.org/abs/2301.05217" target="_blank">Here</a></li>
  </ul>
<br>

<p class="notes-date">
<i>Created: 3rd April, 2025</i>
</p>

   </body>

</html>
