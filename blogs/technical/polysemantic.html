<!DOCTYPE html>
<html lang="en">
<head>
    <title>Anju Chhetri</title>
    <meta
      charset="utf-8"
      name="viewport"
      content="width=device-width, initial-scale=1.0"
    />

    <link
      href="../../css/per_blog.css"
      media="screen"
      rel="stylesheet"
      type="text/css"
    />
  </head>


  <body>
    <article>
        <h1 class="blog-title">Polysemantic Neurons</h1>
        <div class="blog-meta">Published on: October 6, 2024 | Updated: 3rd April, 2025</div>
        
        <hr>
        <i>While conducting a literature review, I came across a word that caught my attention and seemed quite interesting. 
            I thought it would be worthwhile to introduce some relevant terminologies here.</i>
        <hr><br>

        <div class="blog-content">

        When neurons in a neural network correspond to more than one feature, they are called <b>polysemantic neurons</b>. Conversely, if they correspond to only a single feature, they are referred to as <b>monosemantic neurons</b>. <br>
        <br>
        The word 'feature' wasn't easy for me to decode at first. While features can be described as properties interpretable by humans, it cannot be guaranteed that all features are understandable. What may appear as a random signal to a human could represent an important piece of information that human knowledge has failed to decode. 
        One definition of a feature in thWhy reverse engineer? Humans and neural networks often use different representations. For example, while humans solve modular addition using simple carries, a small transformer model learned a Fourier transform strategy insteade context of neural networks that I find fulfilling is:
        <br>
        <br>
        <center>
            Properties of the input which a sufficiently large neural network will reliably dedicate a neuron to representing. For example, curve detectors appear to reliably occur across sufficiently sophisticated vision models, and so are a feature. [1]
        </center>
        <br>
        One interesting finding was that polysemantic neurons tend to occur when the input data exhibits high kurtosis or sparsity. Additionally, their prevalence varies according to the type of architecture used [2]. Features with similar importance levels are often in superposition, whereas features with high importance are more commonly assigned to a dedicated neuron.

        <h3>Polysemanticity and Superposition</h3>
        <br>

        <figure >
            <img class="notes-jpeg-name" id="me" src="/blogs/technical/images/polysemanticity/polyandsuper.png" style="width:500px;height:auto;">
            <figcaption>Fig 1: Visualization of polysemanticity and superposition state in neural networks</figcaption>
        </figure> 

        <br>
        <!-- While going through the literature, I found that these two terms may sound similar, but there is a sharp distinction between them. In polysemanticity, the learned features are not basis-aligned, even if there is an incentive to be so. 
        In contrast, in the superposition state, the number of features encoded by the layer exceeds the number of dimensions it has. Therefore, I like to view superposition as a special case of polysemanticity. -->
        Polysemanticity describes the observed property where learned features within a neural network are not basis-aligned, often encoding multiple, distinct concepts.
        Superposition, on the other hand, is a phenomenon where a layer attempts to represent more features than its dimensionality allows. Consequently, this packing of features into a limited space inevitably leads to a high degree of polysemanticity.
        Thus, superposition can be understood as a specific instance where the phenomenon drives the observed property of polysemanticity to an extreme.

        <br>
        <h3>Circuits</h3>
        Circuits in neural networks are graphs responsible for performing a single task, composed of interconnected neurons that process input signals 
        to produce output. To obtain these circuits, we can use techniques like Layer-wise Relevance Propagation (LRP), which operates by propagating predictions backward through the network. This method helps identify which input features are most relevant to the model's predictions.

        <figure >
            <img class="notes-jpeg-name" id="me" src="/blogs/technical/images/polysemanticity/LRP.png" style="width:500px;height:auto;">
            <figcaption>Fig 2: Layer-wise Relevance Propagation (LRP) in action. (I forgot the website name but will try to remember and cite it.)</figcaption>
        </figure> 


        <br>
        Explainable AI demos: <a href="https://lrpserver.hhi.fraunhofer.de/" target="_blank">Here</a>
        <br>

        <h2>References</h2>
        <ul>
            <li> Toy Models of Superposition | <a href="https://transformer-circuits.pub/2022/toy_model/index.html" target="_blank">Here</a></li>
            <li> Polysemanticity and Capacity in Neural Networks | <a href="https://arxiv.org/abs/2210.01892" target="_blank">Here</a></li>
        </ul>

        </div>
        <button class="back-button" onclick="history.back()">‚Üê Back</button>

    </article>
</body>
</html>