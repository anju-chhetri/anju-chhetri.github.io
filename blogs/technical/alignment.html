<!DOCTYPE html>
<html lang="en">
<head>
    <title>Anju Chhetri</title>
    <meta
      charset="utf-8"
      name="viewport"
      content="width=device-width, initial-scale=1.0"
    />

    <link
      href="../../css/per_blog.css"
      media="screen"
      rel="stylesheet"
      type="text/css"
    />
  </head>


  <body>
    <article>
        <h1 class="blog-title">AI Alignment</h1>
        <div class="blog-meta">Published on: July 19, 2024</div>
        
        <hr>
        <i> Some notes from AI alignment class</i>
        <hr><br>

        <div class="blog-content">
 <b> AI alignment: </b> A subfield of AI safety which I like to view from the lens of robustness, 
    scalable oversight, and Mechanistic Interpretability. I try to avoid the use of alignment with human 
    values because different humans can have different values which can conflict sometimes.
    <br>
    <br>
    Types of AI alignment:
    <ul>
      <li>Outer Alignment: If a system scores high against the reward function but behaves in a way that is not aligned with the true intentions of the creator.
          Failure to define a reward funtion that truely/hollistically captures the human intetion/goal/objective could lead the system optimizing for targets that are only proxy of what we actually want.
      </li>
      <li>
        Inner alignment: If a system fails to achieve the goals/objectives that it was designed for then its an inner alignment problem. This could occur due to problem in objective setting or optimization.
      </li>
    </ul>
    <figure >
      <img class="notes-jpeg-name" id="me" src="/blogs/technical/images/AIAlignment/aiAlignmentTypes.png" style="width:500px;height:auto;">
      <figcaption>Fig 1: Types of AI alignment [1]</figcaption>
    </figure> 
 <b>Goodhart's law:</b> 
 <ul>
<li>When a measure becomes a target, it ceases to be a good measure.</li>
<li>Any observed statistical regularity will tend to collapse when pressure is placed upon it for control purpose.</li>
</ul>

<b>Qualia</b>
<br>
It is subjective, individual experiences of perception and sensation.
For example, when you see a red apple, the redness you perceive is a quale. Even if we understand the neural processes and wavelengths 
of light involved in seeing red, it doesn't fully explain the subjective experience of redness itself.
  
<br>
<br>
<b>Four background claims (Machine Intelligent Research Institute (MIRI)) [2]</b>
<ul>
  <li>Humans have very general ability and can achieve goals across various domains. --> Intelligence/General Intelligence</li>
  <li>AI systems could become much more intelligent than humans. </li>
  <li>If we create highly intelligent AI systems, their decision will shape the future.</li>
  <li>Highly intelligent system won't be beneficial by default.</li>
</ul>
<b>To sum-up:</b>
<br>
These four claims form the core of the argument that artificial intelligence is important: there is 
such a thing as general reasoning ability; if we build general reasoners, they could be far smarter 
than humans; if they are far smarter than humans, they could have an immense impact; and that impact 
will not be beneficial by default.

<h2>References</h2>
  <ul>
    <li>AI safety fundamentals | <a href="https://aisafetyfundamentals.com/blog/what-is-ai-alignment/" target="_blank">Here</a></li>
    <li>Four Background Claims | <a href="https://intelligence.org/2015/07/24/four-background-claims/" target="_blank">Here</a></li>
  </ul>

        </div>
    
        <button class="back-button" onclick="history.back()">‚Üê Back</button>

    </article>
</body>
</html>