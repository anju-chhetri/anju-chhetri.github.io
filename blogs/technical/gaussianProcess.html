<!DOCTYPE html>
<html lang="en">
<head>
    <title>Anju Chhetri</title>
    <meta
      charset="utf-8"
      name="viewport"
      content="width=device-width, initial-scale=1.0"
    />

    <link
      href="../../css/per_blog.css"
      media="screen"
      rel="stylesheet"
      type="text/css"
    />
  </head>


  <body>
    <article>
        <h1 class="blog-title">Gaussian Process</h1>
        <div class="blog-meta">Published on: January 21, 2024</div>
            
        <hr>
        <i>"Let the data speak, if you bring subjective modelling 
            then it’s dangerous."</i> - Someone in YouTube
         <hr><br>

        <div class="blog-content">

            To find the non linear relationship between the variables 
            we can use non-linear functions such as a polynomial or exponential 
            function. But If we don’t have information on the nature of data then 
            finding the appropriate function can be challenging. For this we can use 
            non-parametric methods like the Gaussian process(GP).
            <br>
            <br>
            <i>“The idea of Gaussian process modelling is, without parameterizing y(x)(Prediction function), to place a 
            prior P(y(x)) directly on the space of functions.”</i> - David J.C. Mackay
            <br>
            <br>
            GP is a method of supervised machine learning which is used for regression 
            and probabilistic classification. It provides the <b>distribution over 
            the samples of prediction functions</b>. It also gives uncertainties present in the prediction.
            <br>   
            <br>
            To call it nonparametric would be a misnomer. Nonparametric does not imply the absence of parameters but 
            rather predictions are obtained without giving the unknown function y(x) an explicit parameterization. The parameters 
            present usually increase infinitetly as the model sees 
            more data.
          
            <br>
            <br>
            We can view it as a generalisation of the multivariate gaussian distribution over the infinite dimension. 
            Infinite dimension here means that we have a prediction function that can map infinitely many input values 
            to outputs. As gaussian distribution is characterised by mean and covariance matrix, 
            GP is also characterised by mean function and a covariance function which is given by a kernel.
            <br>
            <math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="script">P</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>&#xA0;</mo><mo>&#x223c;</mo><mo>&#xA0;</mo><mi>G</mi><mi>P</mi><mo>(</mo><mi>m</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>,</mo><mo>&#xA0;</mo><mi>k</mi><mo>(</mo><mi>x</mi><mo>,</mo><mo>&#xA0;</mo><msup><mi>x</mi><mo>'</mo></msup><mo>)</mo><mo>)</mo></math>

            <br>
            <br>
            We keep the mean function zero as the mean does not carry any important information and it’s 
            the covariance function that captures the information required for modelling the GP. Consequently, 
            the covariance matrix determines which type of functions from the space of all possible functions are more probable.
            <br>
            <br>
            <hr>
            <h3>Kernel Function</h3>
            <div class = "subdivison">
               Kernel is responsible for giving similarity between two variables. We can have functions like RBF, Periodic, linear or 
               combination of kernels. Prediction functions that are likely to be sampled are controlled by the kernel.
               <br>
               <br>
               The kernel function recives two points <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>t</mi><mo>,</mo><mo>&#xA0;</mo><msup><mi>t</mi><mo>'</mo></msup><mo>&#xA0;</mo><mo>&#x2208;</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup></math> as an input and returns a similarity score between those two in the form of scaler.
               <br>
               <br>
               <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi><mo>:</mo><mo>&#xA0;</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>&#xA0;</mo><mi>X</mi><mo>&#xA0;</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>&#x2192;</mo><mi mathvariant="double-struck">R</mi><mo>,</mo><mo>&#xA0;</mo><mo>&#x2211;</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mi>c</mi><mi>o</mi><mi>v</mi><mo>(</mo><mi>X</mi><mo>,</mo><mo>&#xA0;</mo><msup><mi>X</mi><mo>'</mo></msup><mo>)</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mi>k</mi><mo>(</mo><mi>t</mi><mo>,</mo><mo>&#xA0;</mo><msup><mi>t</mi><mo>'</mo></msup><mo>)</mo></math>
            </div>
            <br>
            <hr>
            An important property of gaussian distribution that makes GP possible is that the gaussian distribution is closed 
            under marginalisation and conditioning. Which means that the resulting distribution from these operations are also gaussian and 
            this is an important property as this ensures that the obtained results are mathematically tractable.

            <h3>Marginalisation</h3>
            <div class="subdivison">
               Marginalization is summing out the probability of random variable X, 
               given the joint probability distribution of X with other variables.
               <br>
               <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>)</mo><mo>&#xA0;</mo><mo>=</mo><munderover accent='false' accentunder='false'><mo>&#x2211;</mo><mn>0</mn><mi>n</mi></munderover><mi>P</mi><mo>(</mo><mi>X</mi><mo>=</mo><mi>x</mi><mo>|</mo><mo>&#xA0;</mo><msub><mi>Y</mi><mi>n</mi></msub><mo>)</mo></math>
               <br>
            </div>
            <h3>Conditioning</h3>
            <div class="subdivison">
               Conditioning determines the probability of one variable depending on another variable. 
               This allows us to perform Bayesian inference. Through conditioning, we can update our 
               prior beliefs to obtain new distribution as we observe new data points. 
               <br>
               <br>
               <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>(</mo><mi>Y</mi><mo>|</mo><mi>X</mi><mo>)</mo><mo>&#xA0;</mo><mo>=</mo><mo>&#xA0;</mo><mi>P</mi><mo>(</mo><mi>X</mi><mo>&#x2229;</mo><mi>Y</mi><mo>)</mo><mo>/</mo><mi>P</mi><mo>(</mo><mi>X</mi><mo>)</mo></math>

            </div>

        </div>

        <h2>References</h2>
        <ul>   
  
      </li>
          <li>
            A Visual Exploration of Gaussian Processes |  <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/" target="_blank">Here</a> 
          </li>
          <li>
            Gaussian Processes | <a href="https://www.youtube.com/watch?v=UBDgSHPxVME&t=769s" target="_blank">Here</a>
          </li>         
          <li>
            Introduction to Gaussian Processes | <a href="https://d2l.ai/chapter_gaussian-processes/gp-intro.html" target="_blank">Here</a>
          </li>

</ul> 

        </div>
        <button class="back-button" onclick="history.back()">← Back</button>

    </article>
</body>
</html>